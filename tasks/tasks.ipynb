{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f1f286",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cbdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "dl = Downloader(email_address=\"svyoma0604@gmail.com\", company_name=\"Uppsala Student\")\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\", \"TSLA\", \"NVDA\", \"V\", \"JPM\", \"JNJ\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    dl.get(\"10-K\", ticker, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a94ea3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed AAPL: 230963 characters\n",
      "‚úÖ Parsed MSFT: 462372 characters\n",
      "‚úÖ Parsed GOOG: 396386 characters\n",
      "‚úÖ Parsed AMZN: 322545 characters\n",
      "‚úÖ Parsed META: 536345 characters\n",
      "‚úÖ Parsed TSLA: 430601 characters\n",
      "‚úÖ Parsed NVDA: 381765 characters\n",
      "‚úÖ Parsed V: 452899 characters\n",
      "‚úÖ Parsed JPM: 1543510 characters\n",
      "‚úÖ Parsed JNJ: 535435 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "filings_dir = \"./sec-edgar-filings\"\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\", \"TSLA\", \"NVDA\", \"V\", \"JPM\", \"JNJ\"]\n",
    "parsed_docs = []\n",
    "\n",
    "def extract_10k_text(file_content):\n",
    "    \"\"\"Extracts <TEXT> section from the 10-K document block\"\"\"\n",
    "    # Split into <DOCUMENT> blocks\n",
    "    docs = re.findall(r\"<DOCUMENT>(.*?)</DOCUMENT>\", file_content, re.DOTALL | re.IGNORECASE)\n",
    "    for doc in docs:\n",
    "        doc_type_match = re.search(r\"<TYPE>\\s*10-K\", doc, re.IGNORECASE)\n",
    "        if doc_type_match:\n",
    "            text_match = re.search(r\"<TEXT>(.*?)</TEXT>\", doc, re.DOTALL | re.IGNORECASE)\n",
    "            if text_match:\n",
    "                raw_text = text_match.group(1)\n",
    "                return raw_text\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags and extra spaces\"\"\"\n",
    "    # Optional: remove all HTML tags if it's HTML\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)  # crude tag stripper\n",
    "    text = re.sub(r\"\\s+\", \" \", text)      # collapse whitespace\n",
    "    return text.strip()\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        glob_path = f\"{filings_dir}/{ticker}/10-K/0*/full-submission.txt\"\n",
    "        matches = glob.glob(glob_path)\n",
    "        if not matches:\n",
    "            print(f\"‚ùå No 10-K filing found for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        file_path = matches[0]\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        extracted = extract_10k_text(file_content)\n",
    "        if not extracted:\n",
    "            print(f\"‚ö†Ô∏è 10-K <TEXT> block not found for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        cleaned = clean_text(extracted)\n",
    "        parsed_docs.append({\"ticker\": ticker, \"text\": cleaned})\n",
    "        print(f\"‚úÖ Parsed {ticker}: {len(cleaned)} characters\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Error parsing {ticker}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46a7e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for item in parsed_docs:\n",
    "    chunks = text_splitter.create_documents([item[\"text\"]], metadatas=[{\"source\": item[\"ticker\"]}])\n",
    "    docs.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c65d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4fecd6aafd4235ad37e9eda70571e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3537f9c7254b849548cf8f85cc59d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229380df6de446478845125c59d9eab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447639b7c8424cebad59689b82c031cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ae186618794da6ae36680405585c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafa7b679b8348a98f9bc61c0a6270dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f28a75b550646aa936b68f7a3d98d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bb657d577d482994d128dec6dc78d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf8dbb826f4432eb58dee33dedf50cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a896664b994e30b77ae04f5d2ec5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a619a50d919842238bb052b97ba2a402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f74fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "634b4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"faiss_index_10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8d08ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})  # adjust `k` based on recall vs. precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25d2d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "llm_pipeline = pipeline(\"summarization\", model=\"t5-base\", device=0, max_new_tokens=512)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0282bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compressed_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compressed_retriever,\n",
    "    return_source_documents=True  # for cite-aware answers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce7d7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: disasters could affect demand for the Company&#8217;s products and services . armed conflicts and terrorist attacks could affect u.s. and other parts of world . failure to maintain and enhance our brands could harm our business, reputation .\n",
      "\n",
      "Source: JNJ\n",
      "Chunk:\n",
      "remember, *DO NOT* edit the extracted parts of the context . disasters could affect demand for the Company&#8217;s products and services . armed conflicts and terrorist attacks could affect u.s. and other parts of world ....\n",
      "\n",
      "Source: NVDA\n",
      "Chunk:\n",
      "extract any part of context *AS IS* that is relevant to answer the question . if none of the context is relevant return NO_OUTPUT . other risks, trends and uncertainties may also harm our business ....\n",
      "\n",
      "Source: GOOG\n",
      "Chunk:\n",
      "extract any part of context *AS IS* that is relevant to answer the question . if none of the context is relevant return NO_OUTPUT . failure to maintain and enhance our brands could harm our business, reputation ....\n"
     ]
    }
   ],
   "source": [
    "query = \"What does google list as its three primary sources of revenue?\"\n",
    "query_2 = \"Summarize the biggest risk Google cites about supply chain concentration.\"\n",
    "result = qa_chain.invoke({\"query\": query_2})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\\nChunk:\\n{doc.page_content[:300]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
